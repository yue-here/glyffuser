{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Glyffuser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training parameters\n",
    "We add text_encoder and encoder_dim parameters to the config. The batch size is also smaller due to the larger size of the conditional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128  # the generated image resolution\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    num_epochs = 100\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"glyffuser\"  # the model name\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    text_encoder = \"google-t5/t5-small\"\n",
    "    encoder_dim = 1024\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional plumbing has been moved to the `glyffuser_utils` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\y_w_u\\anaconda3\\envs\\torchcuda12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from glyffuser_utils import Collator, GlyffuserPipeline, evaluate\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,                                                                                       \n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps, \n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # Prepare everything for accelerator\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    global_step = 0\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch[0]\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            with accelerator.accumulate(model):\n",
    "                noise_pred = model(\n",
    "                    noisy_images, \n",
    "                    timesteps, \n",
    "                    encoder_hidden_states=batch[1],\n",
    "                    return_dict=False\n",
    "                    )[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = GlyffuserPipeline(\n",
    "                unet=accelerator.unwrap_model(model),\n",
    "                scheduler=inference_scheduler)            \n",
    "            texts=[ # Provides some text prompts for sampling\n",
    "                \"vicious, cruel; severely, extreme\",\n",
    "                \"panting of cow; grunting of ox\",\n",
    "                \"sing; folksong, ballad; rumor\",\n",
    "                \"a quiver\"\n",
    "            ]\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                pipeline = GlyffuserPipeline(unet=accelerator.unwrap_model(model), scheduler=inference_scheduler)\n",
    "                pipeline.save_pretrained(config.output_dir) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a collator which helps the dataloader pass text embeddings to the model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118,923,137 total parameters.\n"
     ]
    }
   ],
   "source": [
    "# Define collator\n",
    "collator = Collator(\n",
    "    image_size=128,\n",
    "    text_label='caption',  # This is where you specify which field to use for text\n",
    "    image_label='file_name',\n",
    "    name=config.text_encoder,\n",
    "    channels='L'\n",
    ")\n",
    "\n",
    "# Define data source\n",
    "dataset = load_dataset(\"json\", data_files=\"data/metadata.jsonl\")\n",
    "train_dataloader = DataLoader(dataset['train'], batch_size=config.train_batch_size, collate_fn=collator, shuffle=True)\n",
    "\n",
    "# Define model\n",
    "model = UNet2DConditionModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=1,  # the number of input channels, 1 for RGB images\n",
    "    out_channels=1,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    # block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    block_out_channels=(128, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "\n",
    "    addition_embed_type=\"text\", # Make it conditional\n",
    "    cross_attention_dim=config.encoder_dim,\n",
    "    encoder_hid_dim=config.encoder_dim,  # the hidden dimension of the encoder\n",
    "    encoder_hid_dim_type=\"text_proj\",  # the hidden dimension of the encoder\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"DownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define optimizers and schedulers\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "inference_scheduler = DPMSolverMultistepScheduler()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")\n",
    "# Check model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass everything to accelerate and run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
